{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "type": "object",
  "properties": {
    "id": {
      "type": "string",
      "description": "Unique object identifier. The format and length of IDs may change over time."
    },
    "content": {
      "type": "array",
      "description": "Content generated by the model. This is an array of content blocks, each of which has a type that determines its shape. If the request input messages ended with an assistant turn, then the response content will continue directly from that last turn. You can use this to constrain the model's output.",
      "items": {
        "type": "object",
        "properties": {
          "type": {
            "type": "string",
            "description": "The type of content block. Can be 'text', 'thinking', 'tool_use', 'tool_result', or other content block types."
          },
          "text": {
            "type": "string",
            "description": "The text content of the message block. Present when type is 'text'."
          }
        }
      }
    },
    "model": {
      "type": "string",
      "description": "The model that handled the request.",
      "minLength": 1,
      "maxLength": 256
    },
    "role": {
      "type": "string",
      "description": "Conversational role of the generated message. This will always be 'assistant'.",
      "enum": ["assistant"]
    },
    "stop_reason": {
      "type": ["string", "null"],
      "description": "The reason that we stopped. This may be one of the following values: 'end_turn' (the model reached a natural stopping point), 'max_tokens' (we exceeded the requested max_tokens or the model's maximum), 'stop_sequence' (one of your provided custom stop_sequences was generated), 'tool_use' (the model invoked one or more tools), 'pause_turn' (we paused a long-running turn. You may provide the response back as-is in a subsequent request to let the model continue), 'refusal' (when streaming classifiers intervene to handle potential policy violations), or 'model_context_window_exceeded'. In non-streaming mode this value is always non-null. In streaming mode, it is null in the message_start event and non-null otherwise.",
      "enum": [
        "end_turn",
        "max_tokens",
        "stop_sequence",
        "tool_use",
        "pause_turn",
        "refusal",
        "model_context_window_exceeded",
        null
      ]
    },
    "stop_sequence": {
      "type": ["string", "null"],
      "description": "Which custom stop sequence was generated, if any. This value will be a non-null string if one of your custom stop sequences was generated."
    },
    "type": {
      "type": "string",
      "description": "Object type. For Messages, this is always 'message'.",
      "enum": ["message"],
      "default": "message"
    },
    "usage": {
      "type": "object",
      "description": "Billing and rate-limit usage. Anthropic's API bills and rate-limits by token counts, as tokens represent the underlying cost to our systems. Under the hood, the API transforms requests into a format suitable for the model. The model's output then goes through a parsing stage before becoming an API response. As a result, the token counts in usage will not match one-to-one with the exact visible content of an API request or response. For example, output_tokens will be non-zero, even for an empty string response from Claude. Total input tokens in a request is the summation of input_tokens, cache_creation_input_tokens, and cache_read_input_tokens.",
      "properties": {
        "input_tokens": {
          "type": "number",
          "description": "The number of input tokens used in the request."
        },
        "output_tokens": {
          "type": "number",
          "description": "The number of output tokens generated in the response."
        },
        "cache_creation_input_tokens": {
          "type": "number",
          "description": "The number of input tokens used to create the cache (if caching was used)."
        },
        "cache_read_input_tokens": {
          "type": "number",
          "description": "The number of input tokens read from the cache (if caching was used)."
        }
      }
    },
    "context_management": {
      "type": ["object", "null"],
      "description": "Context management response. Information about context management strategies applied during the request."
    },
    "container": {
      "type": ["object", "null"],
      "description": "Information about the container used in this request. This will be non-null if a container tool (e.g. code execution) was used. Information about the container used in the request (for the code execution tool)."
    }
  }
}
